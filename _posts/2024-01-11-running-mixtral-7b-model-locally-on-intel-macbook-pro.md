---
layout: post
title: "Running Mixtral 7b LLM Locally on Intel Macbook pro"
summary: "Running Mixtral 7b LLM Locally on Intel Macbook pro"
author: uright
date: '2024-01-11 1:17:00 -0500'
category: ['ml']
tags: ml
thumbnail: /assets/img/posts/code.jpg
keywords: mixtral-7b, llm, intel macbook, opensource
usemathjax: false
permalink: /blog/running-mixtral-7b-llm-locally-on-intel-macbook-pro/
comments: true
---

I'm running a top line MacBook Pro 2019 which runs on Intel CPU along with an AMD graphics card (AMD Radeon Pro 5300M 4GB). I've recently discover a way to run some of the quantized open source LLM model and would like to share some scripts I used to get it working.
